# SV4DA

<img src="./README.assets/SvBench.pdf" alt="SvBench" style="zoom: 130%;"/>

***SvBench*** is a free, powerful benchmark providing a series of accurate or approximate calculation algorithms of Shapley Value(SV) for data analysis(DA). As shown in the figure, it provides a high-level calculation architecture, consisting of the config loader, the sampler, the utility calculator, the convergence checker, and the output aggregator, to calculate Shapley Value for different task targets.

A cooperative game is composed of a player set and a utility function that defines the utility of each coalition (i.e., a subset of the player set). Shapley Value (SV) is a solution concept in cooperative game theory, designed for fairly allocate the total utility generated by the collective efforts of all players within a game. SV has already been widely applied in various DA tasks modeled as cooperative games.

About the use of ***SvBench***, we provide five SV calculation algorithms(**MC**, **RE**, **MLE**, **CP**, and **GT**) and three DA tasks as benchmark as follows:

-   **Data Valuation(DV)** to value each tuple in the dataset based on their contributions to the trained model's performace.
-   **Federated Learning(FL)** to value each local model based on their contributions to the global model's performance.
-   **Result Interpretation(RI)** to interpret the trained model's output result based on the contributions of each feature in dataset to the result.

The datasets, player set, and utility function used for each task are as shown below:

|  Task  | Datasets             | Player       | Utility       | model               |
| :----: | -------------------- | ------------ | ------------- | ------------------- |
| **DV** | *Iris* \ *Wine*      | data tuple   | Test Accuracy | *regression* \ *NN* |
| **FL** | *MNIST* \ *Cifar-10* | data Feature | Test Accuracy | *CNN*               |
| **RI** | *Iris* \ *Wine*      | local model  | Model Output  | *regression* \ *NN* |

For more theoretical details and principles of ***SvBench***, please refer to our survey paper.

[![LICENSE](https://img.shields.io/badge/License-Apache%202.0-green.svg)](https://github.com/apecloud/foxlake/blob/main/LICENSE) [![GitHub CodeQL Advanced](https://github.com/DDDDDstar/SV4DA/actions/workflows/codeql.yml/badge.svg)](https://github.com/DDDDDstar/SV4DA/actions/workflows/codeql.yml) [![Contributors](https://img.shields.io/github/contributors/DDDDDstar/SV4DA?color=3ba272)](https://github.com/DDDDDstar/SV4DA/graphs/contributors) [![Language](https://img.shields.io/badge/Language-Python-blue.svg)](https://www.python.org/)

## Get Started

To use ***SvBench***, you need first to download all the codes to your project directory, which is supposed to be looked like this:

```
.
├── Tasks
│   ├── Nets.py
│   ├── data_preparation.py
│   ├── data_valuation.py
│   ├── federated_learning.py
│   └── result_interpretation.py
├── calculator.py
├── config.py
├── output.py
├── privacy_utils.py
├── run.py
└── sampler.py
```

After this, to run the three benchmark tasks, you could just run the `run.py` directly with relevant parameters.

For example, run the DV task with *Iris* dataset, *regression* model, and **MC** algorithm to calculate SVs:

```sh
python -u run.py --task=DV --dataset=iris --algo=MC --ep=30 --bs=16 --lr=0.01
```

In addition to running benchmark tasks, SvBench also supports users to implement their own DA tasks by parameter settings and remaking the **Sampler** and **Calculator** modules.

The specific parameter settings are explained as follows.

## Parameters

### Parameters for benchmark tasks

To run the benchmark tasks of ***SvBench***, there are the following parameters:

|    Parameter    |         Scope          | Introduction                                                 | Default | Applicable Algorithms |
| :-------------: | :-------------------------------: | ------------------------------------------------------------ | :-----: | --------------- |
|   task   |                 {`DV` `FL` `RI`}                 | The name of the benchmark task. |    -    | - |
| dataset |                 {`iris` `wine`} for **DV** and **RI** tasks, {`mnist` `cifar`} for **FL** task                 | The dataset for tasks. |    -    | - |
|     algo     | {`MC` `RE` `MLE` `GT` `CP`} | The algorithm used to calculate SV. Five algorithms of Monte Carlo random sampling(`MC`), regression-based SV formulation(`RE`), multilinear-extension-based SV formulation(`MLE`), group-testing-based SV formulation(`GT`) and compressive-permutation-based SV formulation(`CP`) are provided. |  `MC`   | - |
|   convergence_threshold    |                             0~1                              | Convergence threshold to check the resultant SVs.            |   0.1    | - |
|             ep             |                              -                               | The number of epoch for ML in utility computation.           |    30    | - |
|             bs             |                              -                               | The batch size for ML in utility computation.                |    16    | - |
| lr | 9 | The learning rate for ML in utility computation. | 0.01 | - |
| sampling_strategy | `random` `antithetic` `stratified` | Three sampling strategies of random sampling, antithetic sampling and stratified sampling are provided to reduce the approximate error. | `random` | - |
| truncation | `True` `False` | Whether to truncate the unnecessary calculations of some marginal contributions in runtime of approximating SV. | `False` | - |
| truncation_threshold | - | Threshold setted in truncation to influence the extent of truncating unnecessary calculations using this technology. | 0.01 | - |
| privacy_protection_measure | `DP` `QT` `DR` | The measure to protect privacy. Three methods of differential privacy(`DP`), quantization(`QT`) and dimension reduction(`DR`) are provided. | `None` | - |
| privacy_protection_level | 0 ~ 1 | The intensity level of providing privacy protection measures, 1 for the highest intensity protection, 0 for non-protection. | 0.0 | - |
| num_parallel_threads | - | The number of threads used for parallel computing. | 1 | - |
| MLE_maxInterval | - | The maximum interval that can be reached in MLE to limit the running time of the algorithm. | 10000 | `MLE` |
| GT_epsilon | - | The epsilon used in GT method. | 0.00001 | `GT` |
| CP_epsilon | - | The epsilon used in CP method. | 0.00001 | `CP` |
| num_measurement | - | The number of measurement. | 10 | `CP` |
|  |  |  |  |  |
|  |  |  |  |  |

## Parameters for User-specific

|    Parameter     | Type | Examples              | Note                                                         |
| :--------------: | :--: | --------------------- | ------------------------------------------------------------ |
|       task       | str  | "feature_attribution" | There must be a `{task}.py` under the `Tasks/` directory.    |
| utility_function | str  | "{task}.utility_calc" | There must be a `{task}.utility_calc` function in `{task}.py`. |
|       algo       | str  | "newSV"               |                                                              |
|                  |      |                       |                                                              |
|                  |      |                       |                                                              |
|                  |      |                       |                                                              |
|                  |      |                       |                                                              |
|                  |      |                       |                                                              |
|                  |      |                       |                                                              |

