# **_SvBench_**

This repository [introduces **_SvBench_**](#using), a benchmark framework for developing Shapley Value (SV) applications in the data analytics (DA) domain, and [provides the instructions for the experiments](#Experiments) presented in [a comprehensive survey of SV in DA](https://arxiv.org/abs/2412.01460).

**_SvBench_** is a free, powerful benchmark providing a series of algorithms for exact computing and approximate computing of Shapley Value in data analytics. SV is a solution concept from cooperative game theory. A cooperative game is composed of a player set and a utility function that defines the utility of each coalition (i.e., a subset of the player set). SV is designed for fairly allocating the overall utility generated by the collective efforts of all players within a game. This solution concept has already been widely applied in various DA tasks modeled as cooperative games for pricing, selection, weighting, and attrition of data and its derivatives (e.g., ML models well trained in DA tasks).

<img src="./README.assets/SvBench.png" alt="SvBench" style="zoom:30%;" />

As shown in the figure, **_SvBench_** is composed of a config loader, a sampler, a utility calculator, a convergence checker, and an output aggregator for computing SV by iterative rounds. A round of SV calculation is conducted starting from the sampler and ending at the convergence checker. Once the convergence criterion is not met, another round will be initiated as demonstrated in the figure (with dashed arrow). The following two tables summarize the functions of the five modules in **_SvBench_** and the main parameters used by SV computing algorithms. Using the five modules, **_SvBench_** implements five base SV calculation algorithms (**MC**, **RE**, **MLE**, **GT**, and **CP**) and several hybrid algorithms, each combining one base algorithm with a specific efficiency optimization. For more details of SV computing techniques, please refer to the [survey paper](https://arxiv.org/abs/2412.01460).

|          Module          | Description                                                                                                                                                                                         | Main Implemented Techniques                                                                               |
| :----------------------: | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------- |
| **configuration loader** | Load the SV computing parameters specified by the users                                                                                                                                             | /                                                                                                         |
|       **sampler**        | Generate the coalitions or permutations of players based on the configured sampling strategy                                                                                                        | Random Sampling, Stratified Sampling, Antithetic Sampling                                                 |
|  **utility calculator**  | Compute the utility of the sampled coalitions or permutations. When users specify an efficiency optimization strategy, the utility calculator will use that strategy to accelerate the computation. | Truncation, ML Speedup for Efficiency Optimization                                                        |
| **convergence checker**  | Determine whether to terminate the SV computation based on the convergence criterion specified in the configuration                                                                                 | SV Ranking                                                                                                |
|  **output aggregator**   | Generate the final SV of each player. If users specify privacy protection measures, the aggregator will execute those measures before reporting the final results.                                  | Measures (i.e., Differential Privacy, Quantization and Dimension Reduction) for Privacy Protection on SV. |

|      Config      | Options (Default setting is in bold.)                                   |
| :--------------: | ----------------------------------------------------------------------- |
|     **Algo**     | **`MC`**, `RE`, `MLE`, `GT`, `CP`, `user-specific`                      |
|   **Sampling**   | `None`,**`random`**, `antithetic`, `stratified`, `user-specific`        |
| **Optimization** | **`None`**, `TC`, `GA`, `TC+GA`, `GA+TSS`, `TC+GA+TSS`, `user-specific` |
|   **Privacy**    | **`None`**, `DP`, `QT`, `DR`, `user-specific`                           |

[![LICENSE](https://img.shields.io/badge/License-Apache%202.0-green.svg)](https://github.com/apecloud/foxlake/blob/main/LICENSE) [![GitHub CodeQL Advanced](https://github.com/DDDDDstar/SV4DA/actions/workflows/codeql.yml/badge.svg)](https://github.com/DDDDDstar/SV4DA/actions/workflows/codeql.yml) [![Contributors](https://img.shields.io/github/contributors/DDDDDstar/SV4DA?color=3ba272)](https://github.com/DDDDDstar/SV4DA/graphs/contributors) [![Language](https://img.shields.io/badge/Language-Python-blue.svg)](https://www.python.org/)

## <span id="using"> Get Started </span>

### Code Preparation

To use **_SvBench_**, users need to download all the codes to their project directory, which is supposed to be looked like this:

```
.
├── Tasks
│   ├── data_preparation.py
│   ├── data_valuation.py
│   ├── dataset_valuation.py
│   ├── federated_learning.py
│   ├── nets.py
│   ├── result_interpretation.py
│   └── utils.py
├── calculator.py
├── config.py
├── output.py
<!--├── privacy.py- 这个模块就不需要了，直接放进output.py作为一个单独的class-->
├── sampler.py
└── svbench.py
```

### Downloading dependencies

```
pip3 install -r requirements.txt
```

### DA Task Preparation and Cooperative Game Modeling

Before using **_SvBench_** to implement a specific SV computing algorithm, users need to properly prepare their targeted DA task, specifying the player and utility function in the task for modeling the task as a cooperative game. Guidelines for cooperative game modelling can be found in our [survey paper](https://arxiv.org/abs/2412.01460).

In this repository, we show three typical DA tasks, namely **Result Interpretation(RI)**, **Data Valuation(DV)**, and **Federated Learning(FL)**, as example use cases. The datasets used by each task (which are generated by `Task/data_preparation.py`) and the cooperative game modelling for each task are summarized in the following tables:

|   Dataset    | # Training Data Tuples | # Test Data Tuples | # Features for Each Tuple | # Classes |
| :----------: | ---------------------- | ------------------ | ------------------------- | --------- |
|   **Iris**   | 120                    | 30                 | 4                         | 3         |
|   **Wine**   | 142                    | 36                 | 13                        | 3         |
|  **MNIST**   | 60,000                 | 10,000             | 1 x 28 x 28               | 10        |
| **Cifar-10** | 50,000                 | 10,000             | 3 x 32 x 32               | 10        |

|  Task  | Dataset    | Player (Number of Players) | Utility       | Model                          |
| :----: | ---------- | -------------------------- | ------------- | ------------------------------ |
| **RI** | _Iris_     | data feature （n=4）       | Model Output  | _Multilayer Perceptron_        |
| **RI** | _Wine_     | data feature （n=13）      | Model Output  | _Multilayer Perceptron_        |
| **DV** | _Iris_     | data tuple （n=120）       | Test Accuracy | _Multilayer Perceptron_        |
| **DV** | _Wine_     | data tuple （n=142）       | Test Accuracy | _Multilayer Perceptron_        |
| **FL** | _MNIST_    | ML model （n=10）          | Test Accuracy | _Convolutional Neural Network_ |
| **FL** | _Cifar-10_ | ML model （n=10）          | Test Accuracy | _Convolutional Neural Network_ |

-   The RI task on Iris uses SV to explain how the four biological features influence the results of classifying three types of iris plants.
-   The RI task on Wine uses SV to explain how 13 chemical constituents influence the results of classifying three types of wine.
-   The DV tasks on Iris and Wine use SV to evaluate the importance of 120 iris plants and 142 wine samples to improve the classification accuracy.
-   The FL tasks on MNIST and Cifar-10 distribute the two datasets to 10 devices, using SV to valuate the local models trained by those devices for the higher accuracy in handwritten digits classification and object recognition.

### SV Computation

Given a well-prepared DA task, users then need to specify the following parameters for implementing a specific SV computing algorithm in the prepared task:

|         Parameter          |                       Scope                        | Description                                                                                                                                                                                                                                                                              | Default  | Type  |
| :------------------------: | :------------------------------------------------: | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------: | :---: |
|            task            |                   `RI` `DV` `FL`                   | The name of the DA task.                                                                                                                                                                                                                                                                 |    -     |  str  |
|          dataset           |           `iris` `wine` `mnist` `cifar`            | The dataset used by DA tasks.                                                                                                                                                                                                                                                            |    -     |  str  |
|         base_algo          |             `MC` `RE` `MLE` `GT` `CP`              | The base SV computing algorithms implemented based on the classical SV formulation (`MC`), regression-based SV formulation (`RE`), multilinear-extension-based SV formulation (`MLE`), group-testing-based SV formulation (`GT`) and compressive-permutation-based SV formulation(`CP`). |   `MC`   |  str  |
|     sampling_strategy      |     `None` `random` `antithetic` `stratified`      | The strategy for sampling coalitions or permutations. `None` for **EXACT** calculation of SV.                                                                                                                                                                                            | `random` |  str  |
|   optimization_strategy    | `None`, `TC`, `GA`, `TC+GA`, `GA+TSS`, `TC+GA+TSS` | Truncation and ML speedup (GA, TSS) techniques for accelerating utility calculation.                                                                                                                                                                                                     |  `None`  |  str  |
|        TC_threshold        |                      $0 - 1$                       | Threshold used by truncation technique.                                                                                                                                                                                                                                                  |   0.01   | float |
| privacy_protection_measure |               `None` `DP` `QT` `DR`                | The measure for handling SV-driven privacy issues. Three measures, i.e., differential privacy(`DP`), quantization(`QT`) and dimension reduction(`DR`), are provided.                                                                                                                     |  `None`  |  str  |
|  privacy_protection_level  |                      $0 - 1$                       | The intensity of privacy protection, 1 for the strongest privacy protection, 0 for no privacy protection.                                                                                                                                                                                |   0.0    | float |
|   convergence_threshold    |                      $0 - 1$                       | Threshold for determining the convergence and termination of SV computing algorithm                                                                                                                                                                                                      |   0.1    | float |
|       conv_check_num       |                 $\mathbb{N}^+\ge2$                 | <!--The number of resultant SVs used to check convergence.--> The number of SV computing rounds on which the convergence criterion is computed                                                                                                                                           |    5     |  int  |
|        manual_seed         |                    $\mathbb{N}$                    | Random seed used throughout the entire SV computing process.                                                                                                                                                                                                                             |    42    |  int  |
|    num_parallel_threads    |                   $\mathbb{N}^+$                   | The number of threads used for parallel computing.                                                                                                                                                                                                                                       |    1     |  int  |
|          log_file          |                         -                          | The file path to save print logs during code running.                                                                                                                                                                                                                                    |  `std`   |  str  |

After the above steps, users can import the `sv_calc` function into their own python file through the command `from svbench import sv_calc` and invoke the `sv_calc` function with the specified parameters for obtaining SV computing results from **_SvBench_**.
Here, we provide some examples of invoking `sv_calc`.

(1) run a base SV computing algorithm, **MC**, in six example DA tasks.

```python
sv_calc(task='RI', dataset='iris', algo='MC')
sv_calc(task='RI', dataset='wine', algo='MC')
sv_calc(task='DV', dataset='iris', algo='MC')
sv_calc(task='DV', dataset='wine', algo='MC')
sv_calc(task='FL', dataset='minst', algo='MC')
sv_calc(task='FL', dataset='cifar', algo='MC')
```

(2) run a hybrid SV computing algorithm, integrating **MC** with **stratified** sampling strategy and **truncation** technique for efficient optimization, in **DV_Iris** task.

```python
sv_calc(task='DV', dataset='iris', algo='MC', sampling_strategy='stratified', optimization_strategy='TC')
```

(2) run a hybrid SV computing algorithm, integrating **MC** with **antithetic** sampling strategy and **truncation** and **gradient-approximation** techniques for efficient optimization, in **DV_Iris** task.

```
pip3 install -r requirements.txt
```

(3) run a hybrid SV computing algorithm, integrating **MC** with **truncation**, **gradient-approximation** and **test-sample-skip** techniques for efficient optimization, in **FL-Cifar** task.

```python
sv_calc(task='FL', dataset='cifar', algo='MC', optimization_strategy='TC+GA+TSS')
```

(4) run a hybrid SV computing algorithm, integrating **MC** with **DP** for privacy protection on SV, in **DV_Iris** task.

|  Task  | Dataset    | Player (Number of Players) | Utility       | Model                          |
| :----: | ---------- | -------------------------- | ------------- | ------------------------------ |
| **RI** | _Iris_     | data feature （n=4）       | Model Output  | _Multilayer Perceptron_        |
| **RI** | _Wine_     | data feature （n=13）      | Model Output  | _Multilayer Perceptron_        |
| **DV** | _Iris_     | data tuple （n=120）       | Test Accuracy | _Multilayer Perceptron_        |
| **DV** | _Wine_     | data tuple （n=142）       | Test Accuracy | _Multilayer Perceptron_        |
| **FL** | _MNIST_    | ML model （n=10）          | Test Accuracy | _Convolutional Neural Network_ |
| **FL** | _Cifar-10_ | ML model （n=10）          | Test Accuracy | _Convolutional Neural Network_ |

-   The RI task on Iris uses SV to explain how the four biological features influence the results of classifying three types of iris plants.
-   The RI task on Wine explains how 13 chemical constituents influence the results of classifying three types of wine.
-   The DV tasks on Iris and Wine use SV to evaluate the importance of 120 iris plants and 142 wine samples to improve the classification accuracy.
-   The FL tasks on MNIST and Cifar-10 distribute the two datasets to 10 devices, using SV to valuate the local models trained by those devices for the higher accuracy in handwritten digits classification and object recognition.

<!-- After this, to run the three benchmark tasks, you could just run the `run.py` directly with relevant parameters.-->

<!-- For example, run the DV task with *Iris* dataset, *regression* model, and **MC** algorithm to calculate SVs:-->

<!-- ```sh-->
<!-- python -u run.py --task=DV --dataset=iris --algo=MC --ep=30 --bs=16 --lr=0.01-->
<!-- ```-->

<!-- In addition to running benchmark tasks, SvBench also supports users to implement their own DA tasks by parameter settings and remaking the **Sampler** and **Calculator** modules.-->

<!-- The specific parameter settings are explained as follows.-->

## SV Computation

Given a well-prepared DA task, users then needs to specify the following parameters used by **_SvBench_** for implement their specific SV computing algorithm in the prepared task:

<!-- ### Parameters for benchmark tasks-->
<!-- To run the benchmark tasks of ***SvBench***, there are the following parameters: -->

|         Parameter          |                       Scope                        | Description                                                                                                                                                                                                                                                                              | Default  | Applicable Algorithms |
| :------------------------: | :------------------------------------------------: | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------: | --------------------- |
|            task            |                   `RI` `DV` `FL`                   | The name of the DA task.                                                                                                                                                                                                                                                                 |    -     | -                     |
|          dataset           |           `iris` `wine` `mnist` `cifar`            | The dataset used by DA tasks.                                                                                                                                                                                                                                                            |    -     | -                     |
|             ep             |                 positive integers                  | The number of epoches for training a model on a specific dataset.                                                                                                                                                                                                                        |    30    | -                     |
|             bs             |                 positive integers                  | The batch sized used in model training .                                                                                                                                                                                                                                                 |    16    | -                     |
|             lr             |                 positive decimals                  | The learning rate used in model training.                                                                                                                                                                                                                                                |   0.01   | -                     |
|         base_algo          |            {`MC` `RE` `MLE` `GT` `CP`}             | The base SV computing algorithms implemented based on the classical SV formulation (`MC`), regression-based SV formulation (`RE`), multilinear-extension-based SV formulation (`MLE`), group-testing-based SV formulation (`GT`) and compressive-permutation-based SV formulation(`CP`). |   `MC`   | -                     |
|         GT_epsilon         |                 positive decimals                  | The epsilon used in GT algorithm.                                                                                                                                                                                                                                                        | 0.00001  | `GT`                  |
|         CP_epsilon         |                 positive decimals                  | The epsilon used in CP algorithm.                                                                                                                                                                                                                                                        | 0.00001  | `CP`                  |
|      num_measurement       |                 positive integers                  | The number of measurements used by CP algorithm.                                                                                                                                                                                                                                         |    10    | `CP`                  |
|   convergence_threshold    |                        0~1                         | Threshold for determining the convergence and termination of SV computing algorithm                                                                                                                                                                                                      |   0.1    | -                     |
|     sampling_strategy      |         `random` `antithetic` `stratified`         | The strategy for sampling coalitions or permutations.                                                                                                                                                                                                                                    | `random` | -                     |
|   optimization_strategy    | `None`, `TC`, `GA`, `TC+GA`, `GA+TSS`, `TC+GA+TSS` | Truncation and ML speedup (GA, TSS) techniques for accelerating utility calculation.                                                                                                                                                                                                     |  `None`  | -                     |
|    truncation_threshold    |                         -                          | Threshold used by truncation technique.                                                                                                                                                                                                                                                  |   0.01   | -                     |
| privacy_protection_measure |                   `DP` `QT` `DR`                   | Differential privacy(`DP`), quantization(`QT`) and dimension reduction(`DR`) for privacy protection on SV.                                                                                                                                                                               |  `None`  | -                     |
|  privacy_protection_level  |                       0 ~ 1                        | The parameter determining the magnitude of privacy protection, 1 for the strongest protection, 0 for no privacy protection.                                                                                                                                                              |   0.0    | -                     |
|    num_parallel_threads    |                         -                          | The number of threads used for parallel computing.                                                                                                                                                                                                                                       |    1     | -                     |

Here, we provide the instructions for running several representative SV computing algorithms in the aforementioned six example DA tasks.
(1) run **MC** algorithm in six DA tasks.

```sh
python -u run.py --task=DV --dataset=iris --algo=MC --ep=30 --bs=16 --lr=0.01
```

```sh
python -u run.py --task=DV --dataset=wine --algo=MC --ep=100 --bs=xxx --lr=xxx
```

## Extend **_SvBench_**

To run <u>user-specific task</u>, there are the following parameters, which users could choose to set to remake the corresponding module:

|         Parameter          |           Scope           | Description                                                                                                                                                                 | Remade modules    | Type     |
| :------------------------: | :-----------------------: | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------- | -------- |
|            task            |             -             | The name of the user-specific task.                                                                                                                                         | calculator        | str      |
|         player_num         |             -             | The number of players in user-specific task.                                                                                                                                | calculator        | int      |
|      utility_function      |             -             | The function for utility computation, which accepts a `list` parameter representing the players coalition to calculate utility and returns a value representing the utility | calculator        | function |
|         base_algo          |             -             | The function for user-specific SV calculation algorithm, which accepts the sampling function result as the parameter.                                                       | calculator        | function |
|          sampling          |             -             | The sampling function for user-specific SV calculation algorithm, which result returned is the input parameter of the algorithm.                                            | sampler           | function |
|         full_check         | `permutation` `coalition` | The object of the convergence check.                                                                                                                                        | output aggregator | str      |
| privacy_protection_measure |             -             | The function for privacy protection for the results.                                                                                                                        |                   | function |
|                            |                           |                                                                                                                                                                             |                   |          |
|                            |                           |                                                                                                                                                                             |                   |          |

## Experiments

Our survey paper uses **_SvBench_** to conduct the following four sets of experiments on the aforementioned example DA tasks in order to answer the questions proposed in the paper:

(1) We compare the efficiency of five base SV computing algorithms with several hybrid algorithms, answering the question **Can the hybrid SV approximation algorithms always ensure higher efficiency than the algorithm using only one of the integrated techniques?**

(2) We investigate the relationship among the computation efficiency, approximation error, and the effectiveness of SV, solving the problem **how to strike a balance between SV approximation error and computation efficiency?**

(3) We examine the effectiveness of existing measures for preventing SV-driven attacks and the impacts of the measures on the effectiveness of SV, tackling the problem **Can a balance be achieved between the effectiveness of privacy protection with the effectiveness of SV?**

(4) We explore the relationship between SV of the four types of players in DA and the overall utility of their associated tasks, offering insights to the question **Can the SV of the four types of players in DA correctly be interpreted by the mainstream paradigm?**

### Instructions for the **first** set of experiments

(1) experiments on **RI_Iris** task

```
# five base algorithms
python exp.py --task=RI --dataset=iris --algo=MC
python exp.py --task=RI --dataset=iris --algo=MLE
python exp.py --task=RI --dataset=iris --algo=RE
python exp.py --task=RI --dataset=iris --algo=GT
python exp.py --task=RI --dataset=iris --algo=CP

# hybrid algorithms
python exp.py --task=RI --dataset=iris --algo=MC --optimization=TC
python exp.py --task=RI --dataset=iris --algo=MLE --optimization=TC
python exp.py --task=RI --dataset=iris --algo=RE --optimization=TC
python exp.py --task=RI --dataset=iris --algo=GT --optimization=TC
python exp.py --task=RI --dataset=iris --algo=CP --optimization=TC
```

(2) experiments on **RI_Wine** task

```
# five base algorithms
python exp.py --task=RI --dataset=wine --algo=MC
python exp.py --task=RI --dataset=wine --algo=MLE
python exp.py --task=RI --dataset=wine --algo=RE
python exp.py --task=RI --dataset=wine --algo=GT
python exp.py --task=RI --dataset=wine --algo=CP

# hybrid algorithms
python exp.py --task=RI --dataset=wine --algo=MC --optimization=TC
python exp.py --task=RI --dataset=wine --algo=MLE --optimization=TC
python exp.py --task=RI --dataset=wine --algo=RE --optimization=TC
python exp.py --task=RI --dataset=wine --algo=GT --optimization=TC
python exp.py --task=RI --dataset=wine --algo=CP --optimization=TC
```

(3) experiments on **DV_Iris** task

```
# five base algorithms
python exp.py --task=DV --dataset=iris --algo=MC
python exp.py --task=DV --dataset=iris --algo=MLE
python exp.py --task=DV --dataset=iris --algo=RE
python exp.py --task=DV --dataset=iris --algo=GT
python exp.py --task=DV --dataset=iris --algo=CP

# hybrid algorithms
python exp.py --task=DV --dataset=iris --algo=MC --optimization=TC
python exp.py --task=DV --dataset=iris --algo=MLE --optimization=TC
python exp.py --task=DV --dataset=iris --algo=RE --optimization=TC
python exp.py --task=DV --dataset=iris --algo=GT --optimization=TC
python exp.py --task=DV --dataset=iris --algo=CP --optimization=TC

python exp.py --task=DV --dataset=iris --algo=MC --optimization=GA
python exp.py --task=DV --dataset=iris --algo=RE --optimization=GA
python exp.py --task=DV --dataset=iris --algo=CP --optimization=GA

python exp.py --task=DV --dataset=iris --algo=MC --optimization=TC+GA
python exp.py --task=DV --dataset=iris --algo=RE --optimization=TC+GA
python exp.py --task=DV --dataset=iris --algo=CP --optimization=TC+GA
```

(4) experiments on **DV_Wine** task

```
# five base algorithms
python exp.py --task=DV --dataset=wine --algo=MC
python exp.py --task=DV --dataset=wine --algo=MLE
python exp.py --task=DV --dataset=wine --algo=RE
python exp.py --task=DV --dataset=wine --algo=GT
python exp.py --task=DV --dataset=wine --algo=CP

# hybrid algorithms
python exp.py --task=DV --dataset=wine --algo=MC --optimization=TC
python exp.py --task=DV --dataset=wine --algo=MLE --optimization=TC
python exp.py --task=DV --dataset=wine --algo=RE --optimization=TC
python exp.py --task=DV --dataset=wine --algo=GT --optimization=TC
python exp.py --task=DV --dataset=wine --algo=CP --optimization=TC

python exp.py --task=DV --dataset=wine --algo=MC --optimization=GA
python exp.py --task=DV --dataset=wine --algo=RE --optimization=GA
python exp.py --task=DV --dataset=wine --algo=CP --optimization=GA

python exp.py --task=DV --dataset=wine --algo=MC --optimization=TC+GA
python exp.py --task=DV --dataset=wine --algo=RE --optimization=TC+GA
python exp.py --task=DV --dataset=wine --algo=CP --optimization=TC+GA
```

(5) experiments on **FL_Mnist** task

```
# five base algorithms
python exp.py --task=FL --dataset=mnist --algo=MC
python exp.py --task=FL --dataset=mnist --algo=MLE
python exp.py --task=FL --dataset=mnist --algo=RE
python exp.py --task=FL --dataset=mnist --algo=GT
python exp.py --task=FL --dataset=mnist --algo=CP

# hybrid algorithms
python exp.py --task=FL --dataset=mnist --algo=MC --optimization=TC
python exp.py --task=FL --dataset=mnist --algo=MLE --optimization=TC
python exp.py --task=FL --dataset=mnist --algo=RE --optimization=TC
python exp.py --task=FL --dataset=mnist --algo=GT --optimization=TC
python exp.py --task=FL --dataset=mnist --algo=CP --optimization=TC

python exp.py --task=FL --dataset=mnist --algo=MC --optimization=GA
python exp.py --task=FL --dataset=mnist --algo=MLE --optimization=GA
python exp.py --task=FL --dataset=mnist --algo=RE --optimization=GA
python exp.py --task=FL --dataset=mnist --algo=GT --optimization=GA
python exp.py --task=FL --dataset=mnist --algo=CP --optimization=GA

python exp.py --task=FL --dataset=mnist --algo=MC --optimization=GA+TSS
python exp.py --task=FL --dataset=mnist --algo=MLE --optimization=GA+TSS
python exp.py --task=FL --dataset=mnist --algo=RE --optimization=GA+TSS
python exp.py --task=FL --dataset=mnist --algo=GT --optimization=GA+TSS
python exp.py --task=FL --dataset=mnist --algo=CP --optimization=GA+TSS

python exp.py --task=FL --dataset=mnist --algo=MC --optimization=TC+GA
python exp.py --task=FL --dataset=mnist --algo=MLE --optimization=TC+GA
python exp.py --task=FL --dataset=mnist --algo=RE --optimization=TC+GA
python exp.py --task=FL --dataset=mnist --algo=GT --optimization=TC+GA
python exp.py --task=FL --dataset=mnist --algo=CP --optimization=TC+GA

python exp.py --task=FL --dataset=mnist --algo=MC --optimization=TC+GA+TSS
python exp.py --task=FL --dataset=mnist --algo=MLE --optimization=TC+GA+TSS
python exp.py --task=FL --dataset=mnist --algo=RE --optimization=TC+GA+TSS
python exp.py --task=FL --dataset=mnist --algo=GT --optimization=TC+GA+TSS
python exp.py --task=FL --dataset=mnist --algo=CP --optimization=TC+GA+TSS
```

(6) experiments on **FL_Cifar** task

```
# five base algorithms
python exp.py --task=FL --dataset=cifar --algo=MC
python exp.py --task=FL --dataset=cifar --algo=MLE
python exp.py --task=FL --dataset=cifar --algo=RE
python exp.py --task=FL --dataset=cifar --algo=GT
python exp.py --task=FL --dataset=cifar --algo=CP

# hybrid algorithms
python exp.py --task=FL --dataset=cifar --algo=MC --optimization=TC
python exp.py --task=FL --dataset=cifar --algo=MLE --optimization=TC
python exp.py --task=FL --dataset=cifar --algo=RE --optimization=TC
python exp.py --task=FL --dataset=cifar --algo=GT --optimization=TC
python exp.py --task=FL --dataset=cifar --algo=CP --optimization=TC

python exp.py --task=FL --dataset=cifar --algo=MC --optimization=GA
python exp.py --task=FL --dataset=cifar --algo=MLE --optimization=GA
python exp.py --task=FL --dataset=cifar --algo=RE --optimization=GA
python exp.py --task=FL --dataset=cifar --algo=GT --optimization=GA
python exp.py --task=FL --dataset=cifar --algo=CP --optimization=GA

python exp.py --task=FL --dataset=cifar --algo=MC --optimization=GA+TSS
python exp.py --task=FL --dataset=cifar --algo=MLE --optimization=GA+TSS
python exp.py --task=FL --dataset=cifar --algo=RE --optimization=GA+TSS
python exp.py --task=FL --dataset=cifar --algo=GT --optimization=GA+TSS
python exp.py --task=FL --dataset=cifar --algo=CP --optimization=GA+TSS

python exp.py --task=FL --dataset=cifar --algo=MC --optimization=TC+GA
python exp.py --task=FL --dataset=cifar --algo=MLE --optimization=TC+GA
python exp.py --task=FL --dataset=cifar --algo=RE --optimization=TC+GA
python exp.py --task=FL --dataset=cifar --algo=GT --optimization=TC+GA
python exp.py --task=FL --dataset=cifar --algo=CP --optimization=TC+GA

python exp.py --task=FL --dataset=cifar --algo=MC --optimization=TC+GA+TSS
python exp.py --task=FL --dataset=cifar --algo=MLE --optimization=TC+GA+TSS
python exp.py --task=FL --dataset=cifar --algo=RE --optimization=TC+GA+TSS
python exp.py --task=FL --dataset=cifar --algo=GT --optimization=TC+GA+TSS
python exp.py --task=FL --dataset=cifar --algo=CP --optimization=TC+GA+TSS
```

### Instructions for the **second** set of experiments

(1) experiments on **RI_Iris** task

```
python exp.py --task=RI --dataset=iris --algo=MC --sampling=random
python exp.py --task=RI --dataset=iris --algo=MC --sampling=stratified
python exp.py --task=RI --dataset=iris --algo=MC --sampling=antithetic
```

(2) experiments on **RI_Wine** task

```
python exp.py --task=RI --dataset=wine --algo=MC --sampling=random
python exp.py --task=RI --dataset=wine --algo=MC --sampling=stratified
python exp.py --task=RI --dataset=wine --algo=MC --sampling=antithetic
```

(3) experiments on **DV_Iris** task

```
python exp.py --task=DV --dataset=iris --algo=MC --sampling=random
python exp.py --task=DV --dataset=iris --algo=MC --sampling=stratified
python exp.py --task=DV --dataset=iris --algo=MC --sampling=antithetic
```

(4) experiments on **DV_Wine** task

```
python exp.py --task=DV --dataset=wine --algo=MC --sampling=random
python exp.py --task=DV --dataset=wine --algo=MC --sampling=stratified
python exp.py --task=DV --dataset=wine --algo=MC --sampling=antithetic
```

(5) experiments on **FL_Mnist** task

```
python exp.py --task=FL --dataset=mnist --algo=MC --sampling=random
python exp.py --task=FL --dataset=mnist --algo=MC --sampling=stratified
python exp.py --task=FL --dataset=mnist --algo=MC --sampling=antithetic
```

(5) experiments on **FL_Cifar** task

```
python exp.py --task=FL --dataset=cifar --algo=MC --sampling=random
python exp.py --task=FL --dataset=cifar --algo=MC --sampling=stratified
python exp.py --task=FL --dataset=cifar --algo=MC --sampling=antithetic
```

### Instructions for the **third** set of experiments

(1) experiments on **RI_Iris** task

```
# differential privacy （protection strength from low to high）
python exp.py --task=RI --dataset=iris --algo=MC --privacy_protection_measure=DP --privacy_protection_level=0.01
python exp.py --task=RI --dataset=iris --algo=MC --privacy_protection_measure=DP --privacy_protection_level=0.05
python exp.py --task=RI --dataset=iris --algo=MC --privacy_protection_measure=DP --privacy_protection_level=0.1

# quantization （protection strength from low to high）
python exp.py --task=RI --dataset=iris --algo=MC --privacy_protection_measure=QT --privacy_protection_level=0.1
python exp.py --task=RI --dataset=iris --algo=MC --privacy_protection_measure=QT --privacy_protection_level=0.5
python exp.py --task=RI --dataset=iris --algo=MC --privacy_protection_measure=QT --privacy_protection_level=0.9


# dimension reduction （protection strength from low to high）
python exp.py --task=RI --dataset=iris --algo=MC --privacy_protection_measure=DR --privacy_protection_level=0.1
python exp.py --task=RI --dataset=iris --algo=MC --privacy_protection_measure=DR --privacy_protection_level=0.5
python exp.py --task=RI --dataset=iris --algo=MC --privacy_protection_measure=DR --privacy_protection_level=0.9
```

(2) experiments on **RI_Wine** task

```
# differential privacy （protection strength from low to high）
python exp.py --task=RI --dataset=wine --algo=MC --privacy_protection_measure=DP --privacy_protection_level=0.01
python exp.py --task=RI --dataset=wine --algo=MC --privacy_protection_measure=DP --privacy_protection_level=0.05
python exp.py --task=RI --dataset=wine --algo=MC --privacy_protection_measure=DP --privacy_protection_level=0.1

# quantization （protection strength from low to high）
python exp.py --task=RI --dataset=wine --algo=MC --privacy_protection_measure=QT --privacy_protection_level=0.1
python exp.py --task=RI --dataset=wine --algo=MC --privacy_protection_measure=QT --privacy_protection_level=0.5
python exp.py --task=RI --dataset=wine --algo=MC --privacy_protection_measure=QT --privacy_protection_level=0.9


# dimension reduction （protection strength from low to high）
python exp.py --task=RI --dataset=wine --algo=MC --privacy_protection_measure=DR --privacy_protection_level=0.1
python exp.py --task=RI --dataset=wine --algo=MC --privacy_protection_measure=DR --privacy_protection_level=0.5
python exp.py --task=RI --dataset=wine --algo=MC --privacy_protection_measure=DR --privacy_protection_level=0.9
```

(3) experiments on **DV_Iris** task

```
# differential privacy （protection strength from low to high）
python exp.py --task=DV --dataset=iris --algo=MC --privacy_protection_measure=DP --privacy_protection_level=0.01
python exp.py --task=DV --dataset=iris --algo=MC --privacy_protection_measure=DP --privacy_protection_level=0.05
python exp.py --task=DV --dataset=iris --algo=MC --privacy_protection_measure=DP --privacy_protection_level=0.1

# quantization （protection strength from low to high）
python exp.py --task=DV --dataset=iris --algo=MC --privacy_protection_measure=QT --privacy_protection_level=0.1
python exp.py --task=DV --dataset=iris --algo=MC --privacy_protection_measure=QT --privacy_protection_level=0.5
python exp.py --task=DV --dataset=iris --algo=MC --privacy_protection_measure=QT --privacy_protection_level=0.9


# dimension reduction （protection strength from low to high）
python exp.py --task=DV --dataset=iris --algo=MC --privacy_protection_measure=DR --privacy_protection_level=0.1
python exp.py --task=DV --dataset=iris --algo=MC --privacy_protection_measure=DR --privacy_protection_level=0.5
python exp.py --task=DV --dataset=iris --algo=MC --privacy_protection_measure=DR --privacy_protection_level=0.9
```

(4) experiments on **DV_Wine** task

```
# differential privacy （protection strength from low to high）
python exp.py --task=DV --dataset=wine --algo=MC --privacy_protection_measure=DP --privacy_protection_level=0.01
python exp.py --task=DV --dataset=wine --algo=MC --privacy_protection_measure=DP --privacy_protection_level=0.05
python exp.py --task=DV --dataset=wine --algo=MC --privacy_protection_measure=DP --privacy_protection_level=0.1

# quantization （protection strength from low to high）
python exp.py --task=DV --dataset=wine --algo=MC --privacy_protection_measure=QT --privacy_protection_level=0.1
python exp.py --task=DV --dataset=wine --algo=MC --privacy_protection_measure=QT --privacy_protection_level=0.5
python exp.py --task=DV --dataset=wine --algo=MC --privacy_protection_measure=QT --privacy_protection_level=0.9


# dimension reduction （protection strength from low to high）
python exp.py --task=DV --dataset=wine --algo=MC --privacy_protection_measure=DR --privacy_protection_level=0.1
python exp.py --task=DV --dataset=wine --algo=MC --privacy_protection_measure=DR --privacy_protection_level=0.5
python exp.py --task=DV --dataset=wine --algo=MC --privacy_protection_measure=DR --privacy_protection_level=0.9
```

### Instructions for the **final** set of experiments

```
python exp.py --task=RI --dataset=iris --algo=MC
python exp.py --task=RI --dataset=wine --algo=MC
python exp.py --task=DV --dataset=iris --algo=MC
python exp.py --task=DV --dataset=wine --algo=MC
python exp.py --task=DSV --dataset=iris --tuple_to_set=12 --algo=MC
python exp.py --task=DSV --dataset=wine --tuple_to_set=15 --algo=MC
python exp.py --task=FL --dataset=mnist --algo=MC
python exp.py --task=FL --dataset=cifar --algo=MC
```

## Citation

If you find our work helpful, please cite us by

```
@misc{lin2024comprehensivestudyshapleyvalue,
      title={A Comprehensive Study of Shapley Value in Data Analytics},
      author={Hong Lin and Shixin Wan and Zhongle Xie and Ke Chen and Meihui Zhang and Lidan Shou and Gang Chen},
      year={2024},
      eprint={2412.01460},
      archivePrefix={arXiv},
      primaryClass={cs.DB},
      url={https://arxiv.org/abs/2412.01460},
}
```
